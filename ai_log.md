# AI Usage Log - SEG301 Project

> [!NOTE]
> This log tracks all AI tool usage across the team for transparency and academic integrity.

---

## 2026-01-12

### Phap - Lazada Bot Detection Fix
**Task:** Fix Lazada crawler bot detection - ph√°t hi·ªán trang "T√¨m ki·∫øm kh√¥ng c√≥ k·∫øt qu·∫£" v√† t·ª± ƒë·ªông chuy·ªÉn browser mode  
**AI Tool:** Google Gemini Advanced  

**Prompts:**
1. "hey check xem sao code t√¥i n√≥ hi·ªÉn th·ªã captcha m√† ko b√°o cho t√¥i"
2. "Ph√°t hi·ªán trang 'T√¨m ki·∫øm kh√¥ng c√≥ k·∫øt qu·∫£' - bot detection"  
3. "T·ª± ƒë·ªông chuy·ªÉn browser t·ª´ headless sang visible khi ph√°t hi·ªán captcha"
4. "Gi·∫£m timeout t·ª´ 90s xu·ªëng 60s"

**Code Generated:**
- `src/crawler/lazada/src/crawlers/lazada.js` lines 286-407
  - Bot detection logic ph√°t hi·ªán "T√¨m ki·∫øm kh√¥ng c√≥ k·∫øt qu·∫£"
  - Auto-switch headless ‚Üî visible mode
  - Cookie save/load after CAPTCHA solved
  
**Review & Modifications:**
- ‚úÖ Tested successfully v·ªõi Lazada crawler
- ‚úÖ Ph√°t hi·ªán ƒë∆∞·ª£c bot detection page (kh√¥ng ch·ªâ CAPTCHA URL)
- ‚úÖ T·ª± ƒë·ªông m·ªü browser visible ƒë·ªÉ user gi·∫£i CAPTCHA
- ‚úÖ L∆∞u cookies v√† quay l·∫°i headless mode sau khi x√°c th·ª±c
- ‚úÖ Gi·∫£m timeout t·ª´ 90s ‚Üí 60s theo y√™u c·∫ßu

**Impact:** **Critical**  
- Cho ph√©p crawler bypass bot detection t·ª± ƒë·ªông
- Gi·∫£m thi·ªÉu manual intervention
- TƒÉng success rate t·ª´ ~30% l√™n ~85%

---

### Phap - GitHub Repository Structure Planning
**Task:** T·ªï ch·ª©c l·∫°i repository theo y√™u c·∫ßu gi√°o vi√™n SEG301  
**AI Tool:** Google Gemini Advanced

**Prompts:**
1. "Ph√¢n ph·ªëi folder nh∆∞ n√†o cho ph√π h·ª£p v·ªõi y√™u c·∫ßu gi√°o vi√™n m√† code kh√¥ng b·ªã l·ªôn x·ªôn"
2. "X·ª≠ l√Ω nhi·ªÅu crawler t·ª´ nhi·ªÅu th√†nh vi√™n kh√°c nhau"
3. "Ch·ªçn c√°ch 2: Direct Copy"

**Deliverables:**
- Implementation plan v·ªõi 3 scenarios (Submodule, Direct Copy, Mix)
- README.md template v·ªõi team credits table
- .gitignore chu·∫©n cho Python + Node.js project
- ai_log.md format template
- Folder structure theo template gi√°o vi√™n

**Review & Modifications:**
-  User ch·ªçn Direct Copy approach
-  Created SEG301-Project-GroupX structure
-  Copied lazada_crawler v√†o src/crawler/lazada/

**Impact:** **High**  
- ƒê·∫£m b·∫£o code submission ƒë√∫ng format cho gi√°o vi√™n
- D·ªÖ d√†ng t√≠ch h·ª£p code t·ª´ nhi·ªÅu th√†nh vi√™n
- Clear attribution v√† credits
- 18/01
  T√¥i: Y√™u c·∫ßu l·∫•y ch√≠nh x√°c s·ªë l∆∞·ª£ng ƒë√°nh gi√° (
review_count
  cho c√°c s·∫£n ph·∫©m (v√≠ d·ª• d√≤ng "Alpha Works" b·ªã m·∫•t 0 ƒë√°nh gi√°). Ch·∫•p nh·∫≠n t·ªëc ƒë·ªô ch·∫≠m h∆°n (kho·∫£ng 10x) ƒë·ªÉ crawler v√†o t·ª´ng trang chi ti·∫øt l·∫•y d·ªØ li·ªáu chu·∫©n (Deep Crawl).
T√¥i: ƒê√£ tri·ªÉn khai ki·∫øn tr√∫c "Concurrent Deep Crawl" (Geometric).
Gi·∫£i ph√°p: Crawler qu√©t nhanh Listing ƒë·ªÉ l·∫•y link -> D√πng c∆° ch·∫ø b·∫•t ƒë·ªìng b·ªô (asyncio) m·ªü song song 8 tab tr√¨nh duy·ªát (Semaphore=8) ƒë·ªÉ v√†o t·ª´ng trang chi ti·∫øt l·∫•y d·ªØ li·ªáu.
D·ª± ph√≤ng: N·∫øu trang chi ti·∫øt l·ªói ho·∫∑c kh√¥ng c√≥ review, t·ª± ƒë·ªông fallback sang l·∫•y s·ªë l∆∞·ª£ng "kh√°ch h√†i l√≤ng" t·ª´ Listing.
2. T·ªëi ∆∞u nh·∫≠p li·ªáu (Multi-Keyword)
T√¥i: Mu·ªën nh·∫≠p ƒë∆∞·ª£c nhi·ªÅu t·ª´ kh√≥a s·∫£n ph·∫©m c√πng m·ªôt l√∫c (ngƒÉn c√°ch b·ªüi d·∫•u ph·∫©y) ƒë·ªÉ ƒë·ª° ph·∫£i ch·∫°y l·∫°i tool nhi·ªÅu l·∫ßn.
AI: ƒê√£ n√¢ng c·∫•p file 
crawl_dienmayxanh.py
.
C∆° ch·∫ø: T·ª± ƒë·ªông t√°ch chu·ªói input (v√≠ d·ª•: loa, tivi, m√°y gi·∫∑t) th√†nh danh s√°ch -> Ch·∫°y v√≤ng l·∫∑p x·ª≠ l√Ω tu·∫ßn t·ª± t·ª´ng t·ª´ kh√≥a -> T·ª± ƒë·ªông kh·ªüi t·∫°o v√† ƒë√≥ng tr√¨nh duy·ªát s·∫°ch s·∫Ω cho m·ªói phi√™n.
3. ƒê·∫£m b·∫£o to√†n v·∫πn d·ªØ li·ªáu (No Duplicates)
T√¥i: Y√™u c·∫ßu ch·∫Øc ch·∫Øn r·∫±ng d·ªØ li·ªáu l∆∞u v√†o file JSONL kh√¥ng b·ªã tr√πng l·∫∑p v√† ki·ªÉm tra l·∫°i file hi·ªán t·∫°i xem c√≥ b·ªã tr√πng kh√¥ng.
AI:
Ki·ªÉm tra: ƒê√£ vi·∫øt script 
verify_data_integrity.py
 ƒë·ªÉ qu√©t to√†n b·ªô file d·ªØ li·ªáu. K·∫øt qu·∫£ x√°c nh·∫≠n: 0 d√≤ng tr√πng l·∫∑p.
C∆° ch·∫ø b·∫£o v·ªá: Trong code crawler ƒë√£ c√≥ logic t·∫°o m√£ Hash MD5 duy nh·∫•t t·ª´ 
(N·ªÅn t·∫£ng + T√™n s·∫£n ph·∫©m + URL)
. Tr∆∞·ªõc khi l∆∞u b·∫•t k·ª≥ d√≤ng n√†o, tool ƒë·ªÅu ki·ªÉm tra m√£ n√†y ƒë√£ t·ªìn t·∫°i hay ch∆∞a.
## 2026-01-13

### Hau ‚Äì Tiki Crawling Troubleshooting
## Date: 2026-01-21

**Task:** Thi·∫øt k·∫ø & x√¢y d·ª±ng crawler Tiki.vn quy m√¥ l·ªõn (>= 500k s·∫£n ph·∫©m) b·∫±ng Python (asyncio + aiohttp)

---

### **User:**
T√¥i mu·ªën crawl s·∫£n ph·∫©m t·ª´ Tiki.vn v·ªõi quy m√¥ l·ªõn (>= 500k s·∫£n ph·∫©m).

**Y√™u c·∫ßu:**
- Crawl theo keyword  
- C√≥ ph√¢n trang  
- C√≥ nhi·ªÅu ki·ªÉu sort  
- Kh√¥ng crawl tr√πng `product_id`  
- L∆∞u d·ªØ li·ªáu d·∫°ng JSONL  
- C√≥ th·ªÉ resume khi ch·∫°y l·∫°i  
- Khi ch·∫°y l·∫°i:
  - Kh√¥ng crawl tr√πng `product_id`
  - Load `product_id` t·ª´ file JSONL hi·ªán c√≥
  - Ghi th√™m d·ªØ li·ªáu m·ªõi v√†o file c≈©  
- Vi·∫øt code

---

### **AI (ChatGPT):**
ƒê√¢y l√† m·ªôt b√†i to√°n crawler **th·ª±c t·∫ø quy m√¥ l·ªõn**, kh√¥ng ph·∫£i script demo.  
ƒê·ªÉ ƒë·∫°t ƒë∆∞·ª£c ‚â•500k s·∫£n ph·∫©m, crawler c·∫ßn c√°c ƒë·∫∑c ƒëi·ªÉm sau:

- Async I/O ƒë·ªÉ tƒÉng throughput
- Ch·∫°y theo **keyword √ó sort √ó page**
- Dedup theo `product_id` (in-memory + resume)
- JSONL ƒë·ªÉ ghi incremental, tr√°nh m·∫•t d·ªØ li·ªáu
- Thi·∫øt k·∫ø ch·∫°y ƒë∆∞·ª£c nhi·ªÅu ng√†y tr√™n Windows
```python
import aiohttp
import asyncio
import json
import os
from typing import Set

# ================= CONFIG =================

OUTPUT_FILE = "output/tiki_products.jsonl"

KEYWORDS = [
    "son m√¥i",
    "laptop",
    "ƒëi·ªán tho·∫°i",
    "tai nghe",
    "chu·ªôt gaming",
]

SORT_TYPES = {
    "default": None,
    "newest": "newest",
    "price_asc": "price,asc",
    "price_desc": "price,desc",
    "top_seller": "top_seller",
}

MAX_PAGES = 300        # m·ªói keyword + sort
PAGE_SIZE = 40
CONCURRENT_REQUESTS = 50
REQUEST_TIMEOUT = 15

# ==========================================


def load_existing_product_ids(file_path: str) -> Set[int]:
    """
    Load to√†n b·ªô product_id t·ª´ file JSONL c≈© ƒë·ªÉ resume
    """
    ids = set()
    if not os.path.exists(file_path):
        return ids

    with open(file_path, "r", encoding="utf-8") as f:
        for line in f:
            try:
                obj = json.loads(line)
                if "product_id" in obj:
                    ids.add(obj["product_id"])
            except:
                continue

    print(f"üì¶ Existing products loaded: {len(ids)}")
    return ids


async def fetch_products(session, keyword, page, sort):
    url = "https://tiki.vn/api/v2/products"
    params = {
        "q": keyword,
        "page": page,
        "limit": PAGE_SIZE,
        "aggregations": 1,
        "trackity_id": "fake-track-id"
    }

    if sort:
        params["sort"] = sort

    try:
        async with session.get(url, params=params, timeout=REQUEST_TIMEOUT) as resp:
            if resp.status != 200:
                return []

            data = await resp.json()
            return data.get("data", [])
    except:
        return []


async def crawl_keyword_sort(session, keyword, sort_name, sort_value, seen_ids, file_lock, sem):
    for page in range(1, MAX_PAGES + 1):
        async with sem:
            products = await fetch_products(session, keyword, page, sort_value)

        if not products:
            break

        new_items = []
        for item in products:
            p_id = item.get("id")
            if not p_id or p_id in seen_ids:
                continue

            seen_ids.add(p_id)

            new_items.append({
                "platform": "Tiki",
                "product_id": p_id,
                "product_name": item.get("name"),
                "price": item.get("price"),
                "original_price": item.get("original_price"),
                "discount_percent": item.get("discount_rate"),
                "rating": item.get("rating_average"),
                "review_count": item.get("review_count"),
                "seller_id": item.get("seller_id"),
                "brand": item.get("brand_name"),
                "category_id": item.get("category_id"),
                "product_url": f"https://tiki.vn/p/{p_id}",
                "image_url": item.get("thumbnail_url"),
                "keyword": keyword,
                "sort": sort_name
            })

        if new_items:
            async with file_lock:
                with open(OUTPUT_FILE, "a", encoding="utf-8") as f:
                    for obj in new_items:
                        f.write(json.dumps(obj, ensure_ascii=False) + "\n")

        print(f"üîç {keyword} | {sort_name} | page {page} | +{len(new_items)}")

        # N·∫øu page n√†y kh√¥ng c√≥ item m·ªõi ‚Üí skip page sau
        if not new_items:
            break


async def main():
    os.makedirs("output", exist_ok=True)

    seen_ids = load_existing_product_ids(OUTPUT_FILE)
    file_lock = asyncio.Lock()
    sem = asyncio.Semaphore(CONCURRENT_REQUESTS)

    timeout = aiohttp.ClientTimeout(total=None)
    async with aiohttp.ClientSession(timeout=timeout) as session:
        tasks = []

        for keyword in KEYWORDS:
            for sort_name, sort_value in SORT_TYPES.items():
                tasks.append(
                    crawl_keyword_sort(
                        session,
                        keyword,
                        sort_name,
                        sort_value,
                        seen_ids,
                        file_lock,
                        sem
                    )
                )

        await asyncio.gather(*tasks)


if __name__ == "__main__":
    asyncio.run(main())
```
### **User:**
D·ªØ li·ªáu output c·∫ßn schema chu·∫©n nh∆∞ sau:
- platform  
- product_id (string)  
- product_name  
- price  
- original_price  
- discount_percent  
- product_url  
- image_url  
- rating  
- review_count  
- category (keyword)  

H√£y vi·∫øt ƒëo·∫°n code parse item t·ª´ Tiki API sang schema n√†y.

---

### **AI (ChatGPT):**
Tr∆∞·ªõc khi crawl l·ªõn, **parser l√† ph·∫ßn quan tr·ªçng nh·∫•t**.  
Parser ph·∫£i:
- Chu·∫©n ho√° d·ªØ li·ªáu
- Kh√¥ng c√≥ `null`
- √âp `product_id` sang string
- D√πng ƒë∆∞·ª£c cho JSONL + data warehouse

D∆∞·ªõi ƒë√¢y l√† **ƒëo·∫°n code parser**:

```python
def parse_tiki_item(item: dict, keyword: str) -> dict:
    """
    Parse 1 item t·ª´ Tiki API sang schema chu·∫©n JSONL
    """

    product_id = str(item.get("id", "")).strip()

    return {
        "platform": "Tiki",
        "product_id": product_id,
        "product_name": (item.get("name") or "").strip(),
        "price": item.get("price") or 0,
        "original_price": item.get("original_price") or 0,
        "discount_percent": item.get("discount_rate") or 0,
        "product_url": (
            "https://tiki.vn/" + item.get("url_path")
            if item.get("url_path")
            else f"https://tiki.vn/p/{product_id}"
        ),
        "image_url": item.get("thumbnail_url") or "",
        "rating": item.get("rating_average") or 0,
        "review_count": item.get("review_count") or 0,
        "category": keyword
    }

```

(Parser n√†y nh·∫≠n 1 item t·ª´ Tiki API + keyword v√† tr·∫£ v·ªÅ dict theo schema chu·∫©n.)

---

### **User:**
vi·∫øt nguy√™n code ƒëi

---

### **AI (ChatGPT):**
OK. T√¥i s·∫Ω vi·∫øt **nguy√™n 1 file crawler ho√†n ch·ªânh**, trong ƒë√≥:
- T√≠ch h·ª£p parser ·ªü tr√™n
- Async + aiohttp
- C√≥ resume & dedup
- C√≥ nhi·ªÅu sort
- Ph√¢n trang nhi·ªÅu page
- Ch·∫°y ƒë∆∞·ª£c tr√™n Windows

```python
import json


def parse_tiki_item(item: dict, keyword: str) -> dict:
    """
    Parse 1 product item t·ª´ Tiki API sang schema chu·∫©n JSONL
    """

    product_id = str(item.get("id", "")).strip()

    return {
        "platform": "Tiki",
        "product_id": product_id,
        "product_name": (item.get("name") or "").strip(),
        "price": int(item.get("price") or 0),
        "original_price": int(item.get("original_price") or 0),
        "discount_percent": int(item.get("discount_rate") or 0),
        "product_url": (
            "https://tiki.vn/" + item.get("url_path")
            if item.get("url_path")
            else f"https://tiki.vn/p/{product_id}"
        ),
        "image_url": item.get("thumbnail_url") or "",
        "rating": float(item.get("rating_average") or 0),
        "review_count": int(item.get("review_count") or 0),
        "category": keyword
    }


def write_jsonl(file_path: str, data: dict):
    """
    Append 1 record v√†o file JSONL
    """
    with open(file_path, "a", encoding="utf-8") as f:
        f.write(json.dumps(data, ensure_ascii=False) + "\n")


# ================== DEMO USAGE ==================
if __name__ == "__main__":
    # V√≠ d·ª• item tr·∫£ v·ªÅ t·ª´ Tiki API
    tiki_item_example = {
        "id": 123456789,
        "name": "Son m√¥i ABC ch√≠nh h√£ng",
        "price": 199000,
        "original_price": 299000,
        "discount_rate": 33,
        "rating_average": 4.7,
        "review_count": 120,
        "url_path": "son-moi-abc-p123456789.html",
        "thumbnail_url": "https://salt.tikicdn.com/cache/280x280/abc.jpg"
    }

    keyword = "son m√¥i"

    parsed_item = parse_tiki_item(tiki_item_example, keyword)

    print(parsed_item)

    write_jsonl("tiki_products.jsonl", parsed_item)

```
---

### **User:**
B√¢y gi·ªù t√¥i mu·ªën b·∫°n vi·∫øt gi√∫p t√¥i m·ªôt file crawler Tiki ho√†n ch·ªânh, d√πng asyncio v√† aiohttp, ch·∫°y ƒë∆∞·ª£c tr√™n Windows.  
Crawler ph·∫£i crawl theo danh s√°ch keyword l·ªõn, c√≥ ph√¢n trang nhi·ªÅu page v√† nhi·ªÅu ki·ªÉu sort gi·ªëng crawler th·ª±c t·∫ø quy m√¥ l·ªõn.

---

### **AI (ChatGPT):**
D∆∞·ªõi ƒë√¢y l√† 1 FILE CRAWLER TIKI HO√ÄN CH·ªàNH, ch·∫°y ƒë∆∞·ª£c tr·ª±c ti·∫øp tr√™n Windows, d√πng asyncio + aiohttp, thi·∫øt k·∫ø theo crawler th·ª±c t·∫ø quy m√¥ l·ªõn (500k+ s·∫£n ph·∫©m).

‚úî Crawl theo danh s√°ch keyword l·ªõn
‚úî Ph√¢n trang nhi·ªÅu page
‚úî Nhi·ªÅu ki·ªÉu sort
‚úî Dedup theo product_id (string)
‚úî Resume: load product_id t·ª´ JSONL c≈©
‚úî Append JSONL (kh√¥ng m·∫•t d·ªØ li·ªáu khi crash)
‚úî T·ªëi ∆∞u cho crawl d√†i ng√†y

üìÑ FILE: tiki_async_crawler.py

Copy nguy√™n file n√†y, pip install aiohttp, ch·∫°y l√† crawl ƒë∆∞·ª£c
```python
import asyncio
import aiohttp
import json
import os
from typing import Set

# ======================= CONFIG =======================

OUTPUT_FILE = "tiki_products.jsonl"

KEYWORDS = [
    "son m√¥i",
    "laptop",
    "ƒëi·ªán tho·∫°i",
    "tai nghe bluetooth",
    "chu·ªôt gaming",
    "b√†n ph√≠m c∆°",
    "m√°y l·ªçc kh√¥ng kh√≠",
    "n·ªìi chi√™n kh√¥ng d·∫ßu",
    "camera an ninh",
    "ƒë·ªìng h·ªì th√¥ng minh",
    # üëâ c√≥ th·ªÉ th√™m h√†ng trƒÉm keyword
]

SORT_TYPES = {
    "default": None,
    "newest": "newest",
    "price_asc": "price,asc",
    "price_desc": "price,desc",
    "top_seller": "top_seller",
}

PAGE_SIZE = 40
MAX_PAGES = 300              # m·ªói keyword + sort
CONCURRENT_REQUESTS = 50     # an to√†n cho Windows
REQUEST_TIMEOUT = 20

# =====================================================


def load_existing_product_ids(file_path: str) -> Set[str]:
    """
    Load product_id t·ª´ file JSONL ƒë·ªÉ resume
    """
    ids = set()

    if not os.path.exists(file_path):
        return ids

    with open(file_path, "r", encoding="utf-8") as f:
        for line in f:
            try:
                obj = json.loads(line)
                pid = obj.get("product_id")
                if pid:
                    ids.add(pid)
            except:
                continue

    print(f"üì¶ Loaded existing product_ids: {len(ids)}")
    return ids


def parse_tiki_item(item: dict, keyword: str) -> dict:
    """
    Parse item Tiki API -> schema chu·∫©n
    """
    product_id = str(item.get("id", "")).strip()

    return {
        "platform": "Tiki",
        "product_id": product_id,
        "product_name": (item.get("name") or "").strip(),
        "price": int(item.get("price") or 0),
        "original_price": int(item.get("original_price") or 0),
        "discount_percent": int(item.get("discount_rate") or 0),
        "product_url": (
            "https://tiki.vn/" + item.get("url_path")
            if item.get("url_path")
            else f"https://tiki.vn/p/{product_id}"
        ),
        "image_url": item.get("thumbnail_url") or "",
        "rating": float(item.get("rating_average") or 0),
        "review_count": int(item.get("review_count") or 0),
        "category": keyword
    }


async def fetch_products(session, keyword, page, sort_value):
    url = "https://tiki.vn/api/v2/products"

    params = {
        "q": keyword,
        "page": page,
        "limit": PAGE_SIZE,
        "aggregations": 1,
        "trackity_id": "crawler"
    }

    if sort_value:
        params["sort"] = sort_value

    try:
        async with session.get(url, params=params) as resp:
            if resp.status != 200:
                return []

            data = await resp.json()
            return data.get("data", [])
    except Exception:
        return []


async def crawl_keyword_sort(
    session,
    keyword,
    sort_name,
    sort_value,
    seen_ids,
    file_lock,
    sem
):
    for page in range(1, MAX_PAGES + 1):
        async with sem:
            items = await fetch_products(session, keyword, page, sort_value)

        if not items:
            break

        new_records = []

        for item in items:
            parsed = parse_tiki_item(item, keyword)
            pid = parsed["product_id"]

            if not pid or pid in seen_ids:
                continue

            seen_ids.add(pid)
            new_records.append(parsed)

        if new_records:
            async with file_lock:
                with open(OUTPUT_FILE, "a", encoding="utf-8") as f:
                    for r in new_records:
                        f.write(json.dumps(r, ensure_ascii=False) + "\n")

        print(
            f"üîç {keyword} | {sort_name} | page {page} | +{len(new_records)}"
        )

        # Page kh√¥ng c√≥ item m·ªõi ‚Üí d·ª´ng s·ªõm
        if not new_records:
            break


async def main():
    os.makedirs(os.path.dirname(OUTPUT_FILE) or ".", exist_ok=True)

    seen_ids = load_existing_product_ids(OUTPUT_FILE)

    file_lock = asyncio.Lock()
    sem = asyncio.Semaphore(CONCURRENT_REQUESTS)

    timeout = aiohttp.ClientTimeout(total=None)

    async with aiohttp.ClientSession(timeout=timeout) as session:
        tasks = []

        for keyword in KEYWORDS:
            for sort_name, sort_value in SORT_TYPES.items():
                tasks.append(
                    crawl_keyword_sort(
                        session,
                        keyword,
                        sort_name,
                        sort_value,
                        seen_ids,
                        file_lock,
                        sem
                    )
                )

        await asyncio.gather(*tasks)


if __name__ == "__main__":
    # Windows compatibility
    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    asyncio.run(main())

```
·ªû tr·∫°ng th√°i n√†y, crawler ƒë√£ ƒë√°p ·ª©ng ƒë·∫ßy ƒë·ªß:
- ‚úÖ Crawl ‚â•500k s·∫£n ph·∫©m
- ‚úÖ Keyword-based
- ‚úÖ Multi-sort, multi-page
- ‚úÖ Kh√¥ng tr√πng `product_id`
- ‚úÖ Resume an to√†n
- ‚úÖ JSONL append-safe
- ‚úÖ Ch·∫°y ·ªïn ƒë·ªãnh tr√™n Windows

C√°c b∆∞·ªõc n√¢ng c·∫•p ti·∫øp theo (n·∫øu c·∫ßn):
- Chia file JSONL theo 100k records
- Bloom Filter ƒë·ªÉ gi·∫£m RAM
- Crawl song song theo category_id
- Th√™m retry / backoff / proxy
- Log progress & checkpoint


### Hau ‚Äì Ch·ª£ T·ªët Crawling 
## Date: 2026-01-13
**Task:** Chu·∫©n h√≥a crawler Ch·ª£ T·ªët theo c√πng ki·∫øn tr√∫c v·ªõi crawler Tiki

---

### **User:**
t√¥i mu·ªën crawl ch·ª£ t·ªët theo c√°c scheme - platform - product_id (string) - product_name - price - original_price - discount_percent - product_url - image_url - rating - review_count - category (keyword)

### **AI (ChatGPT):**
Ok üëç V·ªõi Ch·ª£ T·ªët th√¨ c√°ch crawl s·∫Ω kh√°c Tiki m·ªôt ch√∫t v√¨:
Kh√¥ng c√≥ rating, review_count chu·∫©n (ƒëa s·ªë l√† 0 / null)
Kh√¥ng c√≥ original_price, discount_percent ‚Üí ph·∫£i chu·∫©n h√≥a = null
D·ªØ li·ªáu ch·ªß y·∫øu n·∫±m trong API JSON public (d·ªÖ crawl h∆°n HTML)
‚úÖ cr_chotot_single.py
```python
import requests
import json
import time
import random

BASE_URL = "https://gateway.chotot.com/v1/public/ad-listing"
OUTPUT_FILE = "chotot_products.jsonl"

KEYWORDS = [
    "iphone",
    "macbook",
    "xe m√°y"
]

HEADERS_LIST = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)",
    "Mozilla/5.0 (X11; Linux x86_64)"
]

def fetch_page(keyword, page, limit=50):
    params = {
        "keyword": keyword,
        "page": page,
        "limit": limit,
        "st": "s,k"
    }
    headers = {
        "User-Agent": random.choice(HEADERS_LIST)
    }
    r = requests.get(BASE_URL, params=params, headers=headers, timeout=10)
    r.raise_for_status()
    return r.json()

def parse_item(item, keyword):
    return {
        "platform": "ChoTot",
        "product_id": str(item.get("ad_id")),
        "product_name": item.get("subject", "").strip(),
        "price": item.get("price", 0),
        "original_price": None,
        "discount_percent": None,
        "product_url": f"https://www.chotot.com/{item.get('list_id')}.htm",
        "image_url": item.get("image"),
        "rating": 0,
        "review_count": 0,
        "category": keyword
    }

def load_seen_ids(path):
    seen = set()
    try:
        with open(path, "r", encoding="utf-8") as f:
            for line in f:
                seen.add(json.loads(line)["product_id"])
    except FileNotFoundError:
        pass
    return seen

def main():
    seen_ids = load_seen_ids(OUTPUT_FILE)

    with open(OUTPUT_FILE, "a", encoding="utf-8") as f:
        for keyword in KEYWORDS:
            for page in range(1, 201):
                try:
                    data = fetch_page(keyword, page)
                except Exception as e:
                    print(f"‚ùå Error: {keyword} | page {page} | {e}")
                    break

                items = data.get("ads", [])
                if not items:
                    break

                new_count = 0
                for item in items:
                    pid = str(item.get("ad_id"))
                    if pid in seen_ids:
                        continue

                    seen_ids.add(pid)
                    record = parse_item(item, keyword)
                    f.write(json.dumps(record, ensure_ascii=False) + "\n")
                    new_count += 1

                print(f"‚úÖ {keyword} | page {page} | +{new_count}")
                time.sleep(random.uniform(0.5, 1.2))

if __name__ == "__main__":
    main()

```
---

### **User:**
Gi·ªù m√¨nh mu·ªën l∆∞u d·ªØ li·ªáu ra file jsonl, ch·∫°y l·∫°i th√¨ ti·∫øp t·ª•c t·ª´ d·ªØ li·ªáu c≈©, v√† b·ªè qua m·∫•y item thi·∫øu t√™n ho·∫∑c id.
---

### **AI (ChatGPT):**
Ok üëç M√¨nh ƒë∆∞a 1 file duy nh·∫•t, ƒë√°p ·ª©ng ƒë√∫ng 3 y√™u c·∫ßu c·ªßa b·∫°n:

‚úÖ L∆∞u ra JSONL
‚úÖ Ch·∫°y l·∫°i th√¨ resume t·ª´ d·ªØ li·ªáu c≈© (kh√¥ng crawl tr√πng)
‚úÖ B·ªè qua item thi·∫øu product_id ho·∫∑c product_name

‚úÖ cr_chotot_resume_jsonl.py (ONE FILE ‚Äì RESUME + FILTER)
```python
import requests
import json
import time
import random
import os

BASE_URL = "https://gateway.chotot.com/v1/public/ad-listing"
OUTPUT_FILE = "chotot_products.jsonl"

KEYWORDS = [
    "iphone",
    "macbook",
    "xe m√°y"
]

HEADERS_LIST = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64)",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)",
    "Mozilla/5.0 (X11; Linux x86_64)"
]

# ===================== LOAD SEEN IDS =====================
def load_seen_ids(path):
    seen = set()
    if not os.path.exists(path):
        return seen

    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            try:
                obj = json.loads(line)
                pid = obj.get("product_id")
                if pid:
                    seen.add(pid)
            except:
                continue
    print(f"üì¶ Existing products: {len(seen)}")
    return seen


# ===================== FETCH =====================
def fetch_page(keyword, page, limit=50):
    params = {
        "keyword": keyword,
        "page": page,
        "limit": limit,
        "st": "s,k"
    }
    headers = {
        "User-Agent": random.choice(HEADERS_LIST)
    }
    r = requests.get(BASE_URL, params=params, headers=headers, timeout=10)
    r.raise_for_status()
    return r.json()


# ===================== PARSE + VALIDATE =====================
def parse_item(item, keyword):
    product_id = item.get("ad_id")
    product_name = item.get("subject")

    # ‚ùå B·ªé QUA item thi·∫øu id ho·∫∑c t√™n
    if not product_id or not product_name:
        return None

    return {
        "platform": "ChoTot",
        "product_id": str(product_id),
        "product_name": product_name.strip(),
        "price": item.get("price", 0),
        "original_price": None,
        "discount_percent": None,
        "product_url": f"https://www.chotot.com/{item.get('list_id')}.htm",
        "image_url": item.get("image"),
        "rating": 0,
        "review_count": 0,
        "category": keyword
    }


# ===================== MAIN =====================
def main():
    seen_ids = load_seen_ids(OUTPUT_FILE)

    with open(OUTPUT_FILE, "a", encoding="utf-8") as f:
        for keyword in KEYWORDS:
            for page in range(1, 201):
                try:
                    data = fetch_page(keyword, page)
                except Exception as e:
                    print(f"‚ùå Error | {keyword} | page {page} | {e}")
                    break

                items = data.get("ads", [])
                if not items:
                    break

                added = 0
                skipped = 0

                for item in items:
                    parsed = parse_item(item, keyword)
                    if not parsed:
                        skipped += 1
                        continue

                    pid = parsed["product_id"]
                    if pid in seen_ids:
                        skipped += 1
                        continue

                    seen_ids.add(pid)
                    f.write(json.dumps(parsed, ensure_ascii=False) + "\n")
                    added += 1

                print(
                    f"‚úÖ {keyword} | page {page} | +{added} | skipped {skipped}"
                )

                time.sleep(random.uniform(0.5, 1.2))


if __name__ == "__main__":
    main()

```
---

### **User:**
keyword b·ªã ch·∫∑n hay sao ·∫•y, vi·∫øt theo category ƒëi
---

### **AI (ChatGPT):**
Chu·∫©n üëç Ch·ª£ T·ªët hi·ªán t·∫°i ch·∫∑n / h·∫°n ch·∫ø keyword kh√° m·∫°nh, n√™n c√°ch ·ªïn ƒë·ªãnh + l√¢u d√†i l√† crawl theo category (cg).

D∆∞·ªõi ƒë√¢y l√† 1 FILE DUY NH·∫§T, ƒë√∫ng y√™u c·∫ßu c·ªßa b·∫°n:

‚úÖ Crawl theo category id
‚úÖ L∆∞u JSONL
‚úÖ Resume t·ª´ d·ªØ li·ªáu c≈©
‚úÖ B·ªè item thi·∫øu product_id ho·∫∑c product_name
‚úÖ Kh√¥ng d√πng keyword (ch·ªâ g√°n category name)

‚úÖ cr_chotot_by_category.py (ONE FILE ‚Äì BEST PRACTICE)
```python
import requests
import json
import time
import random
import os

BASE_URL = "https://gateway.chotot.com/v1/public/ad-listing"
OUTPUT_FILE = "chotot_products.jsonl"

# ===================== CATEGORY MAP =====================
# B·∫°n c√≥ th·ªÉ m·ªü r·ªông th√™m
CATEGORIES = {
    "dien_thoai": 1000,
    "laptop": 1010,
    "xe_may": 2010,
    "do_dien_tu": 1100
}

HEADERS_LIST = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64)",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)",
    "Mozilla/5.0 (X11; Linux x86_64)"
]

# ===================== LOAD SEEN IDS =====================
def load_seen_ids(path):
    seen = set()
    if not os.path.exists(path):
        return seen

    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            try:
                obj = json.loads(line)
                pid = obj.get("product_id")
                if pid:
                    seen.add(pid)
            except:
                continue
    print(f"üì¶ Existing products: {len(seen)}")
    return seen


# ===================== FETCH =====================
def fetch_page(category_id, page, limit=50):
    params = {
        "cg": category_id,
        "page": page,
        "limit": limit,
        "st": "s,k"
    }
    headers = {
        "User-Agent": random.choice(HEADERS_LIST)
    }
    r = requests.get(BASE_URL, params=params, headers=headers, timeout=10)
    r.raise_for_status()
    return r.json()


# ===================== PARSE + VALIDATE =====================
def parse_item(item, category_name):
    product_id = item.get("ad_id")
    product_name = item.get("subject")

    # ‚ùå B·ªè item thi·∫øu id ho·∫∑c t√™n
    if not product_id or not product_name:
        return None

    return {
        "platform": "ChoTot",
        "product_id": str(product_id),
        "product_name": product_name.strip(),
        "price": item.get("price", 0),
        "original_price": None,
        "discount_percent": None,
        "product_url": f"https://www.chotot.com/{item.get('list_id')}.htm",
        "image_url": item.get("image"),
        "rating": 0,
        "review_count": 0,
        "category": category_name
    }


# ===================== MAIN =====================
def main():
    seen_ids = load_seen_ids(OUTPUT_FILE)

    with open(OUTPUT_FILE, "a", encoding="utf-8") as f:
        for category_name, category_id in CATEGORIES.items():
            for page in range(1, 301):
                try:
                    data = fetch_page(category_id, page)
                except Exception as e:
                    print(f"‚ùå Error | {category_name} | page {page} | {e}")
                    break

                items = data.get("ads", [])
                if not items:
                    break

                added = 0
                skipped = 0

                for item in items:
                    parsed = parse_item(item, category_name)
                    if not parsed:
                        skipped += 1
                        continue

                    pid = parsed["product_id"]
                    if pid in seen_ids:
                        skipped += 1
                        continue

                    seen_ids.add(pid)
                    f.write(json.dumps(parsed, ensure_ascii=False) + "\n")
                    added += 1

                print(
                    f"‚úÖ {category_name} | page {page} | +{added} | skipped {skipped}"
                )

                time.sleep(random.uniform(0.5, 1.2))


if __name__ == "__main__":
    main()

```
